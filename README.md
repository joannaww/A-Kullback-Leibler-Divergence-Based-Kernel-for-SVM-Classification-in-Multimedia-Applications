# A-Kullback-Leibler-Divergence-Based-Kernel-for-SVM-Classification-in-Multimedia-Applications

Machine learning leverages various scientific disciplines to solve complex problems, with classification being one of the key applications. Support Vector Machines (SVMs), introduced by V. Vapnik, are effective for this task due to their use of kernels, which transform data into higher-dimensional spaces where the data can be separated easier. Traditional kernels such as linear, polynomial, and Gaussian may not capture the complex relationships in multimedia data or may not find application to variable length data. To address this, new kernels such as Fisher and Kullback-Leibler (KL) divergence based kernels have been proposed. In this project, we compare the performance of SVMs using these new kernels against traditional ones in the two-class classification task on the CIFAR-10 dataset.

The project consists of notebooks with SVM experiments with all three types of kernels (Python) and pdf with report from the project.

Authors:
- [Joanna Matuszak](https://github.com/vsiv00)
- Joanna Wojciechowicz
